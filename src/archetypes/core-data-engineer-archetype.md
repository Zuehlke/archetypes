# Core Data Engineer Archetype

The Core Data Engineer represents represents the foundation of data excellence, enabling organizations to leverage the power of data through robust, scalable, and efficient systems. This archetype embodies the essential competencies required to design, build, and maintain data pipelines, storage solutions, data modelling and processing frameworks.

Data Engineers focus on solving real-world problems through data, ensuring reliability, performance, and security while collaborating with data scientists, analysts, and software engineers. Mastery in this role requires not only technical expertise but also a mindset of continuous improvement, automation, and value-driven delivery.

This archetype serves as a roadmap for professionals shaping their careers in data engineering, from foundational skills to advanced mastery.


## Novice
* SQL Basic (simple queries)
* Data Pipeline Basic (Python, PySpark)
* RDBMS Basic (Database, Schema, Tables, Views)
* Data Storage Basic (Stroage Options, File Types)


## Advanced Beginner

* SQL Mastery (complex queries, optimization)
* Data Pipeline Programming (Python, PySpark, or Scala for ETL)
* Batch Processing (Pandas, Spark, cloud ETL tools)
* Database/ Data Storage Options (RDBMS, NoSQL, cloud warehouses)
* Basic Orchestration (Airflow, Luigi)
* Data Quality & Monitoring (validation, alerting)
* Version Control Systems & CI/CD (Git, Jenkins/GitHub Actions)


## Competent

* Advanced SQL (window functions, query tuning)
* DataOps concepts (simple alerts)
* Data modelling patterns (Normalize Forms, OLTP, OLAP)
* Distributed Systems (Spark, partitioning, parallelism)
* Data Streaming (Kafka, Flink, Spark Streaming)
* Cloud-Native Data Platform Architectures (AWS Glue, GCP Dataflow, Azure Data Factory)
* Infrastructure as Code (Terraform, CloudFormation)
* Data Governance (metadata, access controls)

## Proficient

* Scalable Architectures (Lambda/Kappa, data mesh)
* DataOps practices (test automation, operational runbooks)
* Data modelling (Star, dimenaional modelling)
* ML Engineering Collaboration (feature stores, model serving)
* Security & Compliance (GDPR, HIPAA, anonymization)
* Performance Tuning (data pipeline, query)
* Cost Optimization (resource management, cloud efficiency)
* Data Governance (data contracts, data lineage)

## Expert

* Multi-Cloud/Hybrid Architectures
* High-Performance Data Systems (petabyte-scale optimization)
* Economic Impact Analysis (ROI of data investments)
* Industry Thought Leadership (conferences, open-source contributions)
* Executive Communication (translating tech to business value)

